{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from IPython.display import Audio\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from timit_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FrameClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Frame-wise phone classifier for the Timit dataset using RNNs with 29 phonemes\n",
    "    \"\"\"\n",
    "    def __init__(self, num_phonemes=40, hidden_size=64):\n",
    "        super(FrameClassifier, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_phonemes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.num_phonemes = num_phonemes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape [batch_size, seq_len, input_size]\n",
    "        out, _ = self.rnn(x)\n",
    "        # Take the output corresponding to the last input of each sequence\n",
    "        out = out[range(out.shape[0]), -1, :] # [B, T, H] -> [B, H]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=0, step=155, loss=3.1: 100%|██████████| 160/160 [01:51<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "dataset = Timit('timit')\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "model = FrameClassifier()\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = CrossEntropyLoss()\n",
    "epochs = 1\n",
    "num_steps = len(dataloader)*epochs\n",
    "epoch=0\n",
    "with tqdm(range(num_steps)) as pbar:\n",
    "    for step in pbar:\n",
    "        data_path, wav, transcript, x, y = next(iter(dataloader))\n",
    "        x = x.squeeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output.squeeze(0), y.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Report\n",
    "        if step % 5 ==0 :\n",
    "            loss = loss.detach().cpu()\n",
    "            pbar.set_description(f\"epoch={epoch}, step={step}, loss={loss:.1f}\")\n",
    "\n",
    "        if (step+1) % len(dataloader) == 0:\n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 25,  9,  0,  0,  4,  0,  2, 16,  2, 25, 12, 25,  0,  9, 25, 25, 26,\n",
       "        21, 25, 22, 25,  2, 25, 13,  2, 25, 24, 13, 11, 25, 25, 25,  3, 22, 19,\n",
       "         2, 25, 25, 25])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k', 'aa', 's', 'k', 'dh', 'k', 'aa', 'hh', 'er', 'k', 'n', 'er', 'k', 'sh', 'k', 'h#', 'ah', 'k', 'n', 'k', 'k', 'l', 'y', 'k', 'k', 'k', 'er', 'aa', 'ah', 'h#', 'k', 'k', 'aa', 'k', 'ih', 'k', 'k', 'k', 'sh', 'k']\n"
     ]
    }
   ],
   "source": [
    "print([dataset.idx_to_phonemes[i.item()] for i in output.argmax(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 13, 31, 35,  3,  4,  8, 13,  5,  6,  4, 15, 20,  0, 10,  4,  6, 21,\n",
       "        20,  6,  0,  8,  2,  6, 22, 39, 35,  6])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h#', 'd', 'uh', 'z', 'hh', 'ih', 'n', 'd', 'uw', 'h#', 'ih', 'dx', 'iy', 'aa', 'l', 'ih', 'h#', 'jh', 'iy', 'h#', 'aa', 'n', 'er', 'h#', 'k', 'aw', 'z', 'h#']\n"
     ]
    }
   ],
   "source": [
    "print([dataset.idx_to_phonemes[i.item()] for i in y.squeeze(0)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
