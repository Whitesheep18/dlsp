{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      "[[[6 1 4]\n",
      "  [4 8 4]\n",
      "  [6 3 5]\n",
      "  [8 7 9]]\n",
      "\n",
      " [[9 2 7]\n",
      "  [8 8 9]\n",
      "  [2 6 9]\n",
      "  [5 4 1]]]\n"
     ]
    }
   ],
   "source": [
    "#set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 3\n",
    "\n",
    "q = np.random.randint(1, 10, size=(batch_size, seq_len, d_k))\n",
    "k = np.random.randint(1, 10, size=(batch_size, seq_len, d_k))\n",
    "v = np.random.randint(1, 10, size=(batch_size, seq_len, d_k))\n",
    "print('q')\n",
    "print(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[7 8 4 9]\n",
      "  [7 2 6 6]\n",
      "  [9 6 3 1]\n",
      "  [6 8 1 1]]\n",
      "\n",
      " [[1 7 7 2]\n",
      "  [2 5 6 4]\n",
      "  [4 8 6 1]\n",
      "  [8 6 5 1]]]\n",
      "[[[0.0896 0.2436 0.0045 0.6623]\n",
      "  [0.5739 0.0039 0.2111 0.2111]\n",
      "  [0.95   0.0473 0.0024 0.0003]\n",
      "  [0.119  0.8794 0.0008 0.0008]]\n",
      "\n",
      " [[0.0012 0.4977 0.4977 0.0034]\n",
      "  [0.012  0.2418 0.6572 0.0889]\n",
      "  [0.0159 0.8661 0.1172 0.0008]\n",
      "  [0.8431 0.1141 0.042  0.0008]]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return special.softmax(x, axis=-1) # numerically stable\n",
    "    e_x = np.exp(x)\n",
    "    res = e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "scaled = np.random.randint(1, 10, size=(batch_size, seq_len, seq_len))\n",
    "print(scaled)\n",
    "attention = softmax(scaled)\n",
    "print(attention)\n",
    "print(attention.sum(axis=-1, keepdims=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1] # 64 (because 512/8=64)\n",
    "    # k.T would just reverse the shape, but we want to transpose the last two dimentions, so use .transpose(-1, -2)\n",
    "    scaled = np.matmul(q, np.swapaxes(k, -1, -2), ) / np.sqrt(d_k) # scaling will keep the var somewhat close to var(q) (and var(k)) to keep gradient step stable\n",
    "    if mask is not None: # if encoder, mask is None, if decoder, masking future tokens\n",
    "        scaled += mask # elementwise addition\n",
    "    attention = softmax(scaled)\n",
    "    values = np.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "def get_forward_mask(seq_len, eps=-1e9):\n",
    "    # eps essentially negative infinity (after softmax turns to 0)\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)), k=1)*eps\n",
    "    return mask.astype(int)\n",
    "\n",
    "mask = get_forward_mask(seq_len)\n",
    "\n",
    "values, attention = scaled_dot_product(q, k, v, mask)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(x, mask, W_qkv, W_out, num_heads):\n",
    "    batch_size, sequence_length, d_model = x.shape  # batch_size x max_sequence_length x 512\n",
    "    head_dim = d_model // num_heads \n",
    "    qkv = x @ W_qkv                                 # batch_size x max_sequence_length x 1536\n",
    "    qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3*head_dim) # batch_size x max_sequence_length x 8 x 192 (because 3 * 64)\n",
    "    qkv = qkv.transpose(0, 2, 1, 3) # batch_size x 8 x max_sequence_length x 192\n",
    "    q, k, v = np.array_split(qkv, 3, axis=-1) # tuple of 3 tensors (each [batch_size x 8 x max_sequence_length x 64])\n",
    "    values, attention = scaled_dot_product(q, k, v, mask)\n",
    "    values = values.reshape(batch_size, sequence_length, num_heads*head_dim) # like a concatenation along the last axis + swapping the last two axes\n",
    "    out = values @ W_out\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x= np.random.randint(1, 10, size=(batch_size, seq_len, d_k))\n",
    "mask = get_forward_mask(seq_len, -1000_000_000)\n",
    "W_qkv = np.random.randn(*(d_k, 3*d_k))\n",
    "W_out = np.random.randn(*(d_k, d_k))\n",
    "out = multihead_attention(x, mask=mask, W_qkv=W_qkv, W_out=W_out, num_heads=3)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def multihead_crossattention(x, y, mask, W_kv, W_q, W_out, num_heads):\n",
    "    batch_size, sequence_length, d_model = x.shape # bs x max_seq_len x 512\n",
    "    head_dim = d_model // num_heads\n",
    "    kv = x @ W_kv                           # bs x max_seq_len x 1024\n",
    "    q = y @ W_q                             # bs x max_seq_len x 512\n",
    "    kv = kv.reshape(batch_size, sequence_length, num_heads, 2*head_dim) # bs x max_seq_len x 8 x 128\n",
    "    q = q.reshape(batch_size, sequence_length, num_heads, head_dim)     # bs x max_seq_len x 8 x 64\n",
    "    kv = kv.transpose(0, 2, 1, 3)                                       # bs x 8 x max_seq_len x 128\n",
    "    q = q.transpose(0, 2, 1, 3)                                         # bs x 8 x max_seq_len x 64\n",
    "    k, v = np.array_split(kv, 2, axis = -1)                             # tuple of 2 tensors of shape bs x 8 x max_seq_len x 64\n",
    "    values, attention = scaled_dot_product(q, k, v)                     # values: bs x 8 x max_seq_len x 64. We don't need mask here\n",
    "    values = values.reshape(batch_size, sequence_length, d_model)       # concatentate heads: bs x max_seq_len x 512\n",
    "    out = values @ W_out # bs x max_seq_len x 512\n",
    "    return out\n",
    "\n",
    "\n",
    "x = np.random.randint(1, 10, size=(batch_size, seq_len, d_k))\n",
    "y = np.random.randint(1, 10, size=(batch_size, seq_len, d_k))\n",
    "mask = get_forward_mask(seq_len, -1000_000_000)\n",
    "W_kv = np.random.randn(*(d_k, 2*d_k))\n",
    "W_q = np.random.randn(*(d_k, d_k))\n",
    "W_out = np.random.randn(*(d_k, d_k))\n",
    "out = multihead_crossattention(x, y, mask=mask, W_kv=W_kv, W_q=W_q, W_out=W_out, num_heads=3)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder_block(x):\n",
    "    W_qkv = np.random.randn(*(d_k, 3*d_k))\n",
    "    W_out = np.random.randn(*(d_k, d_k))\n",
    "    x = multihead_attention(x, mask=None, W_qkv=W_qkv, W_out=W_out, num_heads=3)\n",
    "    W_ff1 = np.random.randn(*(d_k, 2048))\n",
    "    W_ff2 = np.random.randn(*(2048, d_k))\n",
    "    x = np.maximum(0, x @ W_ff1) @ W_ff2\n",
    "    return x\n",
    "\n",
    "x = np.random.randint(1, 10, size=(batch_size, seq_len, d_k))\n",
    "N = 5\n",
    "for layer_id in range(N):\n",
    "    x = encoder_block(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
