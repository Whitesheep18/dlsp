{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedLibri(LIBRISPEECH):\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sr, transcript, speakerID, chapterID, utteranceID = super().__getitem__(index)\n",
    "        max_samples = 249600 # 15.6 seconds\n",
    "        waveform = waveform[:, :max_samples]  # truncate waveform to size max_samples\n",
    "        if waveform.shape[1] < max_samples:\n",
    "            waveform = torch.cat([waveform, torch.zeros(1, max_samples - waveform.shape[1])], dim=1)\n",
    "        return waveform, sr, transcript, speakerID, chapterID, utteranceID\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TruncatedLibri('.', download=True), batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    waveform, sr, transcript, speakerID, chapterID, utteranceID = next(iter(train_loader))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('May need to downgrade pytorch to 2.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, strides = [5, 2, 2, 2, 2, 2, 2], kernel_sizes = [10, 3, 3, 3, 3, 2, 2]):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        assert len(strides) == len(kernel_sizes), \"strides and kernel_sizes must have the same length\"\n",
    "\n",
    "        num_layers = len(strides)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv1d(1, channels, kernel_size=kernel_sizes[i], stride=strides[i]))\n",
    "            elif i == num_layers - 1:\n",
    "                layers.append(nn.Conv1d(channels, 1, kernel_size=kernel_sizes[i], stride=strides[i]))\n",
    "            else:\n",
    "                layers.append(nn.Conv1d(channels, channels, kernel_size=kernel_sizes[i], stride=strides[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            if i == num_layers - 1:\n",
    "\n",
    "                layers.append(nn.GroupNorm(1, 1))\n",
    "            else:\n",
    "                layers.append(nn.GroupNorm(1, channels))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class Context(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Context, self).__init__()\n",
    "        self.context = torch.nn.Transformer(d_model = 768, nhead=8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.context(x)\n",
    "\n",
    "class ProductQuantization(nn.Module):\n",
    "    \"\"\"This is how it should be:\n",
    "    For the quantization module we use G = 2 and V = 320 resulting in a theoretical maximum of 102.4k codewords. Entries are of size d/G = 128\"\"\"\n",
    "    def __init__(self, num_subvectors, subvector_dim, num_codebooks):\n",
    "        super(ProductQuantization, self).__init__()\n",
    "        self.num_subvectors = num_subvectors\n",
    "        self.subvector_dim = subvector_dim\n",
    "        self.num_codebooks = num_codebooks\n",
    "        \n",
    "        # Initialize codebooks\n",
    "        self.codebooks = nn.Parameter(torch.randn(num_subvectors, num_codebooks, subvector_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, dim = x.shape\n",
    "        assert dim == self.num_subvectors * self.subvector_dim, \\\n",
    "            f\"Input dimension must be equal to num_subvectors * subvector_dim but was {dim} instead of {num_subvectors} x {subvector_dim} = {num_subvectors * subvector_dim}\"\n",
    "        \n",
    "        # Reshape input into subvectors\n",
    "        x = x.view(batch_size, self.num_subvectors, self.subvector_dim)\n",
    "        \n",
    "        # Quantize each subvector independently\n",
    "        quantized_vectors = []\n",
    "        for i in range(self.num_subvectors):\n",
    "            subvector = x[:, i, :]\n",
    "            codebook = self.codebooks[i]\n",
    "            \n",
    "            # Compute distances between subvector and codebook entries\n",
    "            distances = torch.cdist(subvector.unsqueeze(1), codebook.unsqueeze(0))\n",
    "            \n",
    "            # Find nearest codebook entry\n",
    "            indices = torch.argmin(distances, dim=-1)\n",
    "            \n",
    "            # Get quantized vectors from codebook\n",
    "            quantized_vector = codebook[indices]\n",
    "            quantized_vectors.append(quantized_vector)\n",
    "        \n",
    "        # Concatenate quantized subvectors\n",
    "        quantized_vectors = torch.stack(quantized_vectors, dim=1)\n",
    "        quantized_vectors = quantized_vectors.view(batch_size, dim)\n",
    "        \n",
    "        return quantized_vectors\n",
    "\n",
    "class Wav2Vec2(torch.nn.Module):\n",
    "    def __init__(self, encoder_channels, context_channels, num_layers= 7, num_subvectors=19, subvector_dim=41, num_codebooks=256, mask_size=10):\n",
    "        super(Wav2Vec2, self).__init__()\n",
    "        self.encoder = Encoder(encoder_channels)\n",
    "        self.context = Context()\n",
    "        self.quantization = ProductQuantization(num_subvectors, subvector_dim, num_codebooks)\n",
    "        self.mask_feature_vector = nn.Parameter(torch.randn(1, 1, mask_size))\n",
    "        self.mask_size = mask_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        q = self.quantization(z.squeeze(1)).unsqueeze(1)\n",
    "        masked_z = mask(z, self.mask_feature_vector, M=self.mask_size)\n",
    "        c = self.context(masked_z)\n",
    "        return q, c\n",
    "    \n",
    "\n",
    "def mask(z, mask_feature_vector, p = 0.065, M=10):\n",
    "    # sample p indices (timesteps) to be starting indices for mask\n",
    "    T = z.shape[2]\n",
    "    mask_start = torch.randperm(T)[:int(p*T)]\n",
    "    mask = z\n",
    "    for i in mask_start:\n",
    "\n",
    "        if len(mask) >= i+M:\n",
    "            mask[:, :, i:i+M] = mask_feature_vector[:, :, :M]\n",
    "        else:\n",
    "            mask[:, :, i:i+M] = mask_feature_vector\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2(4, 4)\n",
    "q, c = model(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 256\n",
    "G = 2\n",
    "V = 320\n",
    "\n",
    "d/G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cosine similarity\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "cosine_similarity(q, c, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
