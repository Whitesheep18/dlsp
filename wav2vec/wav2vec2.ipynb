{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedLibri(LIBRISPEECH):\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sr, transcript, speakerID, chapterID, utteranceID = super().__getitem__(index)\n",
    "        max_samples = 249600 # 15.6 seconds\n",
    "        waveform = waveform[:, :max_samples]  # truncate waveform to size max_samples\n",
    "        if waveform.shape[1] < max_samples:\n",
    "            waveform = torch.cat([waveform, torch.zeros(1, max_samples - waveform.shape[1])], dim=1)\n",
    "        return waveform, sr, transcript, speakerID, chapterID, utteranceID\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TruncatedLibri('.', download=True), batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    waveform, sr, transcript, speakerID, chapterID, utteranceID = next(iter(train_loader))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('May need to downgrade pytorch to 2.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, strides = [5, 2, 2, 2, 2, 2, 2], kernel_sizes = [10, 3, 3, 3, 3, 2, 2]):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        assert len(strides) == len(kernel_sizes), \"strides and kernel_sizes must have the same length\"\n",
    "\n",
    "        num_layers = len(strides)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv1d(1, channels, kernel_size=kernel_sizes[i], stride=strides[i]))\n",
    "            elif i == num_layers - 1:\n",
    "                layers.append(nn.Conv1d(channels, 1, kernel_size=kernel_sizes[i], stride=strides[i]))\n",
    "            else:\n",
    "                layers.append(nn.Conv1d(channels, channels, kernel_size=kernel_sizes[i], stride=strides[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            if i == num_layers - 1:\n",
    "\n",
    "                layers.append(nn.GroupNorm(1, 1))\n",
    "            else:\n",
    "                layers.append(nn.GroupNorm(1, channels))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class Context(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Context, self).__init__()\n",
    "        self.context = torch.nn.Transformer(d_model = 768, nhead=8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.context(x)\n",
    "\n",
    "class ProductQuantization(nn.Module):\n",
    "    \"\"\"This is how it should be:\n",
    "    For the quantization module we use G = 2 and V = 320 resulting in a theoretical maximum of 102.4k codewords. Entries are of size d/G = 128\"\"\"\n",
    "    def __init__(self, num_subvectors, subvector_dim, num_codebooks):\n",
    "        super(ProductQuantization, self).__init__()\n",
    "        self.num_subvectors = num_subvectors\n",
    "        self.subvector_dim = subvector_dim\n",
    "        self.num_codebooks = num_codebooks\n",
    "        \n",
    "        # Initialize codebooks\n",
    "        self.codebooks = nn.Parameter(torch.randn(num_subvectors, num_codebooks, subvector_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, dim = x.shape\n",
    "        assert dim == self.num_subvectors * self.subvector_dim, \\\n",
    "            f\"Input dimension must be equal to num_subvectors * subvector_dim but was {dim} instead of {num_subvectors} x {subvector_dim} = {num_subvectors * subvector_dim}\"\n",
    "        \n",
    "        # Reshape input into subvectors\n",
    "        x = x.view(batch_size, self.num_subvectors, self.subvector_dim)\n",
    "        \n",
    "        # Quantize each subvector independently\n",
    "        quantized_vectors = []\n",
    "        for i in range(self.num_subvectors):\n",
    "            subvector = x[:, i, :]\n",
    "            codebook = self.codebooks[i]\n",
    "            \n",
    "            # Compute distances between subvector and codebook entries\n",
    "            distances = torch.cdist(subvector.unsqueeze(1), codebook.unsqueeze(0))\n",
    "            \n",
    "            # Find nearest codebook entry\n",
    "            indices = torch.argmin(distances, dim=-1)\n",
    "            \n",
    "            # Get quantized vectors from codebook\n",
    "            quantized_vector = codebook[indices]\n",
    "            quantized_vectors.append(quantized_vector)\n",
    "        \n",
    "        # Concatenate quantized subvectors\n",
    "        quantized_vectors = torch.stack(quantized_vectors, dim=1)\n",
    "        quantized_vectors = quantized_vectors.view(batch_size, dim)\n",
    "        \n",
    "        return quantized_vectors\n",
    "\n",
    "class Wav2Vec2(torch.nn.Module):\n",
    "    def __init__(self, encoder_channels, context_channels, num_layers= 7, num_subvectors=19, subvector_dim=41, num_codebooks=256, mask_size=10):\n",
    "        super(Wav2Vec2, self).__init__()\n",
    "        self.encoder = Encoder(encoder_channels)\n",
    "        self.context = Context()\n",
    "        self.quantization = ProductQuantization(num_subvectors, subvector_dim, num_codebooks)\n",
    "        self.mask_feature_vector = nn.Parameter(torch.randn(1, 1, mask_size))\n",
    "        self.mask_size = mask_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        q = self.quantization(z.squeeze(1)).unsqueeze(1)\n",
    "        masked_z = mask(z, self.mask_feature_vector, M=self.mask_size)\n",
    "        c = self.context(masked_z)\n",
    "        return q, c\n",
    "    \n",
    "\n",
    "def mask(z, mask_feature_vector, p = 0.065, M=10):\n",
    "    # sample p indices (timesteps) to be starting indices for mask\n",
    "    T = z.shape[2]\n",
    "    mask_start = torch.randperm(T)[:int(p*T)]\n",
    "    mask = z\n",
    "    for i in mask_start:\n",
    "\n",
    "        if len(mask) >= i+M:\n",
    "            mask[:, :, i:i+M] = mask_feature_vector[:, :, :M]\n",
    "        else:\n",
    "            mask[:, :, i:i+M] = mask_feature_vector\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m q, c \u001b[38;5;241m=\u001b[39m model(waveform)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlsp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 93\u001b[0m, in \u001b[0;36mWav2Vec2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization(z\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m masked_z \u001b[38;5;241m=\u001b[39m mask(z, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_feature_vector, M\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_size)\n\u001b[0;32m---> 93\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext(masked_z)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q, c\n",
      "File \u001b[0;32m~/miniconda3/envs/dlsp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36mContext.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlsp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2(4, 4)\n",
    "q, c = model(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 256\n",
    "G = 2\n",
    "V = 320\n",
    "\n",
    "d/G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1258],\n",
       "        [0.0693]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cosine similarity\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "cosine_similarity(q, c, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
